services:
  inference:
    build:
      context: ../../../inference-service
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=gpt-4o-mini
      - OPENAI_TEMPERATURE=0
    ports:
      - "8000:8000"

  prioritization:
    build:
      context: ../../..
    depends_on:
      - inference
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_TOPIC_IN=tickets.incoming
      - KAFKA_TOPIC_OUT=tickets.prioritized
      - REMSFAL_CLASSIFIER_MODE=inference
      - REMSFAL_INFERENCE_PROVIDER=baseline
      - REMSFAL_INFERENCE_BASE_URL=http://inference:8000
      - REMSFAL_INFERENCE_TIMEOUT_MS=3000
