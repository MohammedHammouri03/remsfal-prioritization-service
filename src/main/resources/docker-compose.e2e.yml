services:
  inference:
    build:
      context: ../../../inference-service
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=gpt-4o-mini
      - OPENAI_TEMPERATURE=0
    ports:
      - "8000:8000"
    networks:
      - remsfal
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health').read()"]
      interval: 5s
      timeout: 3s
      retries: 20
      start_period: 10s

  prioritization:
    build:
      context: ../../..
    depends_on:
      inference:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-broker:9092
      - KAFKA_TOPIC_IN=tickets.incoming
      - KAFKA_TOPIC_OUT=tickets.prioritized
      - REMSFAL_CLASSIFIER_MODE=inference
      - REMSFAL_INFERENCE_PROVIDER=baseline
      - REMSFAL_INFERENCE_BASE_URL=http://inference:8000
      - REMSFAL_INFERENCE_TIMEOUT_MS=3000
    networks:
      - remsfal

networks:
  remsfal:
    name: remsfal
