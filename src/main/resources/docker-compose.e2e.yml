services:
  inference:
    build:
      context: ../../../inference-service
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=gpt-4o-mini
      - OPENAI_TEMPERATURE=0
    ports:
      - "8000:8000"
    networks:
      - remsfal-backend

  prioritization:
    build:
      context: ../../..
    depends_on:
      inference:
        condition: service_started
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-broker:9092
      - KAFKA_TOPIC_IN=tickets.incoming
      - KAFKA_TOPIC_OUT=tickets.prioritized
      - REMSFAL_CLASSIFIER_MODE=inference
      - REMSFAL_INFERENCE_PROVIDER=baseline
      - REMSFAL_INFERENCE_BASE_URL=http://inference:8000
      - REMSFAL_INFERENCE_TIMEOUT_MS=3000
    networks:
      - remsfal-backend

networks:
  remsfal-backend:
    external: true
    name: remsfal-backend_remsfal
